{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4559ab02",
   "metadata": {},
   "source": [
    "## Whisper Small "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "\n",
    "MODEL_NAME = \"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=\"ro\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "\n",
    "AUDIO_FOLDER = \"\"\n",
    "OUTPUT_FOLDER = \"\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "wav_files = [f for f in os.listdir(AUDIO_FOLDER) if f.endswith(\".wav\")]\n",
    "failed_files = []\n",
    "\n",
    "\n",
    "for file in tqdm(wav_files, desc=\"Transcribing\", unit=\"file\"):\n",
    "    audio_file_path = os.path.join(AUDIO_FOLDER, file)\n",
    "    txt_file_path = os.path.join(OUTPUT_FOLDER, os.path.splitext(file)[0] + \".txt\")\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        audio, sampling_rate = librosa.load(audio_file_path, sr=16000)\n",
    "        \n",
    "\n",
    "        audio_input = processor(audio, return_tensors=\"pt\", sampling_rate=sampling_rate)\n",
    "        input_features = audio_input['input_features'].to(device)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(input_features=input_features)\n",
    "        \n",
    "\n",
    "        transcription = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        with open(txt_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(transcription)\n",
    "    \n",
    "    except Exception as e:\n",
    "        failed_files.append(file)\n",
    "        with open(txt_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\")  \n",
    "\n",
    "if failed_files:\n",
    "    print(\"\\nThe following files failed to transcribe:\")\n",
    "    for file in failed_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"\\nAll files processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caece6b",
   "metadata": {},
   "source": [
    "## Whisper Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b425287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "MODEL_NAME = \"\"\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=\"ro\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "\n",
    "AUDIO_FOLDER = \"\"\n",
    "OUTPUT_FOLDER = \"\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "wav_files = [f for f in os.listdir(AUDIO_FOLDER) if f.endswith(\".wav\")]\n",
    "failed_files = []\n",
    "\n",
    "\n",
    "for file in tqdm(wav_files, desc=\"Transcribing\", unit=\"file\"):\n",
    "    audio_file_path = os.path.join(AUDIO_FOLDER, file)\n",
    "    txt_file_path = os.path.join(OUTPUT_FOLDER, os.path.splitext(file)[0] + \".txt\")\n",
    "    \n",
    "    try:\n",
    "        audio, sampling_rate = librosa.load(audio_file_path, sr=16000)\n",
    "        audio_input = processor(audio, return_tensors=\"pt\", sampling_rate=sampling_rate)\n",
    "        input_features = audio_input[\"input_features\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(input_features, max_new_tokens=200)\n",
    "        \n",
    "        transcription = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        with open(txt_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(transcription)\n",
    "\n",
    "    except Exception as e:\n",
    "        failed_files.append(file)\n",
    "        with open(txt_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\")  \n",
    "\n",
    "\n",
    "if failed_files:\n",
    "    print(\"\\nThe following files failed to transcribe:\")\n",
    "    print(\"\\n\".join(failed_files))\n",
    "else:\n",
    "    print(\"\\nAll files processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78a9b3a",
   "metadata": {},
   "source": [
    "## Wav2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6833370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"\")\n",
    "model = AutoModelForCTC.from_pretrained(\"\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "AUDIO_FOLDER = \"\"  \n",
    "OUTPUT_FOLDER = \"\" \n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "wav_files = [f for f in os.listdir(AUDIO_FOLDER) if f.endswith(\".wav\")]\n",
    "failed_files = []\n",
    "\n",
    "\n",
    "for file in tqdm(wav_files, desc=\"Transcribing\", unit=\"file\"):\n",
    "    audio_path = os.path.join(AUDIO_FOLDER, file)\n",
    "    txt_path = os.path.join(OUTPUT_FOLDER, os.path.splitext(file)[0] + \".txt\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        audio_input, sr = sf.read(audio_path)\n",
    "        if len(audio_input.shape) > 1:\n",
    "            audio_input = audio_input.mean(axis=1) \n",
    "\n",
    "\n",
    "        inputs = processor(audio_input, return_tensors=\"pt\", sampling_rate=16000)\n",
    "        input_values = inputs.input_values.to(device)\n",
    "\n",
    " \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "\n",
    "       \n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "     \n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(transcription)\n",
    "\n",
    "    except Exception as e:\n",
    "        failed_files.append(file)\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\") \n",
    "        print(f\"Failed: {file} â€” {e}\")\n",
    "\n",
    "\n",
    "if failed_files:\n",
    "    print(\"\\nThe following files failed to transcribe:\")\n",
    "    for file in failed_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"\\nAll files processed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
